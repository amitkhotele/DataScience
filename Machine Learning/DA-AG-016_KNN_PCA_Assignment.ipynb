{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a9b27846",
   "metadata": {},
   "source": [
    "## Question 1\n",
    "**What is K-Nearest Neighbors (KNN) and how does it work in both classification and regression problems?**\n",
    "\n",
    "**Answer:**\n",
    "\n",
    "K-Nearest Neighbors (KNN) is a non-parametric, instance-based supervised learning algorithm used for both classification and regression. It makes predictions by finding the `k` training samples closest to a query point (according to a distance metric such as Euclidean or Manhattan) and using their labels to decide the prediction.\n",
    "\n",
    "- **Classification:** The predicted class is typically the majority class among the k nearest neighbors (voting). Optionally, neighbors can be weighted by inverse distance so nearer neighbors contribute more.\n",
    "- **Regression:** The predicted value is usually the mean (or weighted mean) of the target values of the k nearest neighbors.\n",
    "\n",
    "**Key properties:**\n",
    "- Simple, intuitive, and requires no explicit training (training is just storing the dataset).\n",
    "- Sensitive to feature scales — feature scaling (standardization or normalization) is important.\n",
    "- Choice of `k` influences bias-variance: small `k` → low bias, high variance; large `k` → high bias, low variance.\n",
    "- Computationally expensive on large datasets because nearest neighbor search scales with number of samples (though KD-trees, Ball-trees, or approximate methods mitigate this).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47ebab8b",
   "metadata": {},
   "source": [
    "## Question 2\n",
    "**What is the Curse of Dimensionality and how does it affect KNN performance?**\n",
    "\n",
    "**Answer:**\n",
    "\n",
    "The \"Curse of Dimensionality\" refers to phenomena that arise when working with high-dimensional data. As dimensionality (number of features) increases:\n",
    "\n",
    "- Data points become sparse; the volume of the space increases exponentially and points are far apart on average.\n",
    "- Distance metrics become less informative: distances between nearest and farthest neighbors tend to concentrate, reducing contrast.\n",
    "- Models that rely on locality (like KNN) degrade because the concept of \"neighborhood\" becomes less meaningful.\n",
    "- Overfitting risk grows when sample size is small relative to dimensionality.\n",
    "\n",
    "**Impact on KNN:**\n",
    "- KNN relies on distance to define similarity; with many dimensions, distances lose discriminative power and nearest neighbors may not be truly similar.\n",
    "- Performance often worsens; KNN can become noisy and unstable.\n",
    "\n",
    "**Mitigations:**\n",
    "- Dimensionality reduction (PCA, t-SNE, UMAP) or feature selection.\n",
    "- Increase sample size if possible.\n",
    "- Use distance metrics or learned embeddings that better reflect similarity for the problem domain.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e5e4fd1",
   "metadata": {},
   "source": [
    "## Question 3\n",
    "**What is Principal Component Analysis (PCA)? How is it different from feature selection?**\n",
    "\n",
    "**Answer:**\n",
    "\n",
    "Principal Component Analysis (PCA) is a linear dimensionality reduction technique that projects data onto a lower-dimensional orthogonal basis (principal components) chosen to maximize explained variance. Steps:\n",
    "1. Center the data (subtract mean).\n",
    "2. Compute covariance matrix.\n",
    "3. Compute eigenvalues and eigenvectors.\n",
    "4. Sort eigenvectors by eigenvalue (variance explained) and project data onto the top components.\n",
    "\n",
    "**Differences from feature selection:**\n",
    "- **PCA (feature extraction):** Constructs new features (linear combinations of original features). The new features (principal components) are orthogonal and ordered by variance explained. Original features are not preserved directly.\n",
    "- **Feature selection:** Chooses a subset of original features without creating new ones (e.g., filter methods, wrapper methods, embedded methods). Interpretability is usually better for feature selection because original measurements remain.\n",
    "\n",
    "PCA reduces dimensionality while retaining maximum variance (for linear projections), whereas feature selection keeps or discards original features based on some criterion.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2558eb9",
   "metadata": {},
   "source": [
    "## Question 4\n",
    "**What are eigenvalues and eigenvectors in PCA, and why are they important?**\n",
    "\n",
    "**Answer:**\n",
    "\n",
    "In PCA, eigenvectors (principal components) of the covariance matrix define directions in feature space along which the data varies. Eigenvalues correspond to the amount of variance captured along each eigenvector.\n",
    "\n",
    "- **Eigenvector:** A direction (unit vector) in the original feature space. Projecting data onto this vector gives coordinates along that principal axis.\n",
    "- **Eigenvalue:** Scalar that quantifies the variance of the data along its eigenvector.\n",
    "\n",
    "Importance:\n",
    "- Sorting by eigenvalue gives an ordering of principal components by importance (explained variance).\n",
    "- Choosing the top eigenvectors (highest eigenvalues) yields a low-dimensional subspace that preserves most of the variance.\n",
    "- Eigenvalues let us compute the *explained variance ratio* to decide how many components to keep.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9a2ae71",
   "metadata": {},
   "source": [
    "## Question 5\n",
    "**How do KNN and PCA complement each other when applied in a single pipeline?**\n",
    "\n",
    "**Answer:**\n",
    "\n",
    "PCA and KNN are often combined:\n",
    "- **PCA reduces dimensionality**, alleviating the curse of dimensionality, removing noisy or redundant features, and improving distance-based algorithms' reliability.\n",
    "- **KNN benefits from lower-dimensional, decorrelated features** because distances become more meaningful, computation is faster, and overfitting risk lowers.\n",
    "\n",
    "Typical pipeline: scale → PCA (retain components capturing most variance) → KNN (tune `k`, distance metric). This often yields better generalization and faster inference.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16b980c4",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Practical section — Wine dataset experiments\n",
    "\n",
    "We use `sklearn.datasets.load_wine()` and run the following experiments:\n",
    "\n",
    "- Q6: KNN with and without feature scaling\n",
    "- Q7: PCA and explained variance ratios\n",
    "- Q8: KNN on PCA-transformed data (top 2 components)\n",
    "- Q9: KNN with different distance metrics\n",
    "- Q10: Discussion + example pipeline for high-dimensional gene expression data (with demo code)\n",
    "\n",
    "Run the code cells sequentially to reproduce results.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "45bd52e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wine dataset shape: (178, 13)\n",
      "Classes: [0 1 2]\n"
     ]
    }
   ],
   "source": [
    "# Q6-Q9 — experiments with the Wine dataset\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.datasets import load_wine\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "# Load dataset\n",
    "data = load_wine()\n",
    "X = data.data\n",
    "y = data.target\n",
    "feature_names = data.feature_names\n",
    "\n",
    "print('Wine dataset shape:', X.shape)\n",
    "print('Classes:', np.unique(y))\n",
    "\n",
    "# create a DataFrame for easy viewing\n",
    "df = pd.DataFrame(X, columns=feature_names)\n",
    "df['target'] = y\n",
    "\n",
    "# train-test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, stratify=y, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "23510f42",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy without scaling: 0.7778\n",
      "Accuracy with StandardScaler: 0.9333\n",
      "\n",
      "Classification report (scaled model):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00        15\n",
      "           1       0.94      0.89      0.91        18\n",
      "           2       0.85      0.92      0.88        12\n",
      "\n",
      "    accuracy                           0.93        45\n",
      "   macro avg       0.93      0.94      0.93        45\n",
      "weighted avg       0.94      0.93      0.93        45\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Q6: Train KNN classifier WITH and WITHOUT feature scaling and compare accuracy\n",
    "from sklearn.pipeline import make_pipeline\n",
    "\n",
    "# Without scaling\n",
    "knn = KNeighborsClassifier(n_neighbors=5)\n",
    "knn.fit(X_train, y_train)\n",
    "y_pred = knn.predict(X_test)\n",
    "acc_no_scaling = accuracy_score(y_test, y_pred)\n",
    "\n",
    "# With scaling\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "knn_scaled = KNeighborsClassifier(n_neighbors=5)\n",
    "knn_scaled.fit(X_train_scaled, y_train)\n",
    "y_pred_scaled = knn_scaled.predict(X_test_scaled)\n",
    "acc_with_scaling = accuracy_score(y_test, y_pred_scaled)\n",
    "\n",
    "print('Accuracy without scaling: {:.4f}'.format(acc_no_scaling))\n",
    "print('Accuracy with StandardScaler: {:.4f}'.format(acc_with_scaling))\n",
    "\n",
    "# Show brief classification report for scaled model\n",
    "print('\\nClassification report (scaled model):')\n",
    "print(classification_report(y_test, y_pred_scaled))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4846ef7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of components: 13\n",
      "PC1: 0.3620 (cumulative 0.3620)\n",
      "PC2: 0.1921 (cumulative 0.5541)\n",
      "PC3: 0.1112 (cumulative 0.6653)\n",
      "PC4: 0.0707 (cumulative 0.7360)\n",
      "PC5: 0.0656 (cumulative 0.8016)\n",
      "PC6: 0.0494 (cumulative 0.8510)\n",
      "PC7: 0.0424 (cumulative 0.8934)\n",
      "PC8: 0.0268 (cumulative 0.9202)\n",
      "PC9: 0.0222 (cumulative 0.9424)\n",
      "PC10: 0.0193 (cumulative 0.9617)\n",
      "PC11: 0.0174 (cumulative 0.9791)\n",
      "PC12: 0.0130 (cumulative 0.9920)\n",
      "PC13: 0.0080 (cumulative 1.0000)\n",
      "\n",
      "Components needed to retain >=90% variance: 8\n"
     ]
    }
   ],
   "source": [
    "# Q7: Train PCA and print explained variance ratio of each component\n",
    "pca = PCA()\n",
    "X_scaled_full = StandardScaler().fit_transform(X)  # PCA on scaled features\n",
    "pca.fit(X_scaled_full)\n",
    "explained_ratios = pca.explained_variance_ratio_\n",
    "\n",
    "print('Number of components:', len(explained_ratios))\n",
    "for i, ratio in enumerate(explained_ratios, start=1):\n",
    "    cum = explained_ratios[:i].cumsum()[-1]\n",
    "    print(f'PC{i}: {ratio:.4f} (cumulative {cum:.4f})')\n",
    "\n",
    "# Also show cumulative explained variance and components to retain 90% variance\n",
    "cum_var = np.cumsum(explained_ratios)\n",
    "components_90 = np.searchsorted(cum_var, 0.90) + 1\n",
    "print('\\nComponents needed to retain >=90% variance:', components_90)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8bdd8644",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on original scaled data (k=5): 0.9333\n",
      "Accuracy on PCA (2 components) data (k=5): 0.9333\n",
      "\n",
      "Classification report (PCA 2 components):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.93      0.93      0.93        15\n",
      "           1       0.89      0.94      0.92        18\n",
      "           2       1.00      0.92      0.96        12\n",
      "\n",
      "    accuracy                           0.93        45\n",
      "   macro avg       0.94      0.93      0.94        45\n",
      "weighted avg       0.94      0.93      0.93        45\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Q8: KNN on PCA-transformed dataset (top 2 components)\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# Fit scaler on full data split used earlier\n",
    "scaler = StandardScaler().fit(X_train)\n",
    "X_train_s = scaler.transform(X_train)\n",
    "X_test_s = scaler.transform(X_test)\n",
    "\n",
    "pca2 = PCA(n_components=2)\n",
    "X_train_pca2 = pca2.fit_transform(X_train_s)\n",
    "X_test_pca2 = pca2.transform(X_test_s)\n",
    "\n",
    "knn_pca2 = KNeighborsClassifier(n_neighbors=5)\n",
    "knn_pca2.fit(X_train_pca2, y_train)\n",
    "y_pred_pca2 = knn_pca2.predict(X_test_pca2)\n",
    "acc_pca2 = accuracy_score(y_test, y_pred_pca2)\n",
    "\n",
    "print('Accuracy on original scaled data (k=5): {:.4f}'.format(acc_with_scaling))\n",
    "print('Accuracy on PCA (2 components) data (k=5): {:.4f}'.format(acc_pca2))\n",
    "\n",
    "print('\\nClassification report (PCA 2 components):')\n",
    "print(classification_report(y_test, y_pred_pca2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "133764eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metric: euclidean -> Accuracy: 0.9333\n",
      "Metric: manhattan -> Accuracy: 0.9778\n"
     ]
    }
   ],
   "source": [
    "# Q9: KNN with different distance metrics on the scaled Wine dataset\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Correct scaler usage: fit on train, transform on test\n",
    "scaler = StandardScaler().fit(X_train)\n",
    "X_train_s = scaler.transform(X_train)\n",
    "X_test_s = scaler.transform(X_test)\n",
    "\n",
    "for metric in ['euclidean', 'manhattan']:\n",
    "    knn_m = KNeighborsClassifier(n_neighbors=5, metric=metric)\n",
    "    knn_m.fit(X_train_s, y_train)\n",
    "    y_pred_m = knn_m.predict(X_test_s)\n",
    "    acc = accuracy_score(y_test, y_pred_m)\n",
    "    print(f'Metric: {metric:9s} -> Accuracy: {acc:.4f}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a7f3902",
   "metadata": {},
   "source": [
    "## Question 10\n",
    "\n",
    "**Scenario:** High-dimensional gene expression dataset (many features, few samples) causing overfitting.\n",
    "\n",
    "**Explain how you would:**\n",
    "\n",
    "- **Use PCA to reduce dimensionality**\n",
    "  - Standardize features (zero mean, unit variance).\n",
    "  - Fit PCA on training data (not on entire dataset including test) to avoid data leakage.\n",
    "  - Project data onto top principal components that capture most variance.\n",
    "\n",
    "- **Decide how many components to keep**\n",
    "  - Use explained variance ratio and cumulative explained variance (e.g., keep components that capture 90–95% of variance).\n",
    "  - Alternatively, use domain knowledge, scree plot (elbow), cross-validation performance with different component counts, or downstream classification performance.\n",
    "\n",
    "- **Use KNN for classification post-dimensionality reduction**\n",
    "  - Use the PCA-transformed features as input to KNN.\n",
    "  - Tune `k` and distance metric via cross-validation.\n",
    "  - Use distance-weighted voting if useful.\n",
    "\n",
    "- **Evaluate the model**\n",
    "  - Use stratified train-test split or nested cross-validation if sample size permits.\n",
    "  - Evaluate with accuracy, precision, recall, F1-score, ROC-AUC (for binary or one-vs-rest multi-class), and confusion matrices.\n",
    "  - Use permutation tests or repeated CV for robust estimates given small sample size.\n",
    "\n",
    "- **Justify to stakeholders**\n",
    "  - PCA reduces noise and stabilizes distance-based classifiers, lowering overfitting risk.\n",
    "  - The pipeline (scaling → PCA → cross-validated KNN) is transparent and reproducible.\n",
    "  - Components can be inspected (loadings) to relate back to genes if interpretability is needed (rotate or examine feature contributions).\n",
    "  - Provide robust validation (e.g., nested CV) and confidence intervals for metrics.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2f033a73",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Synthetic high-dim data shape: (100, 2000)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\amitk\\AppData\\Roaming\\Python\\Python313\\site-packages\\sklearn\\model_selection\\_validation.py:516: FitFailedWarning: \n",
      "30 fits failed out of a total of 120.\n",
      "The score on these train-test partitions for these parameters will be set to nan.\n",
      "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "30 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\amitk\\AppData\\Roaming\\Python\\Python313\\site-packages\\sklearn\\model_selection\\_validation.py\", line 859, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "    ~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\amitk\\AppData\\Roaming\\Python\\Python313\\site-packages\\sklearn\\base.py\", line 1365, in wrapper\n",
      "    return fit_method(estimator, *args, **kwargs)\n",
      "  File \"C:\\Users\\amitk\\AppData\\Roaming\\Python\\Python313\\site-packages\\sklearn\\pipeline.py\", line 655, in fit\n",
      "    Xt = self._fit(X, y, routed_params, raw_params=params)\n",
      "  File \"C:\\Users\\amitk\\AppData\\Roaming\\Python\\Python313\\site-packages\\sklearn\\pipeline.py\", line 589, in _fit\n",
      "    X, fitted_transformer = fit_transform_one_cached(\n",
      "                            ~~~~~~~~~~~~~~~~~~~~~~~~^\n",
      "        cloned_transformer,\n",
      "        ^^^^^^^^^^^^^^^^^^^\n",
      "    ...<5 lines>...\n",
      "        params=step_params,\n",
      "        ^^^^^^^^^^^^^^^^^^^\n",
      "    )\n",
      "    ^\n",
      "  File \"C:\\Users\\amitk\\AppData\\Roaming\\Python\\Python313\\site-packages\\joblib\\memory.py\", line 312, in __call__\n",
      "    return self.func(*args, **kwargs)\n",
      "           ~~~~~~~~~^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\amitk\\AppData\\Roaming\\Python\\Python313\\site-packages\\sklearn\\pipeline.py\", line 1540, in _fit_transform_one\n",
      "    res = transformer.fit_transform(X, y, **params.get(\"fit_transform\", {}))\n",
      "  File \"C:\\Users\\amitk\\AppData\\Roaming\\Python\\Python313\\site-packages\\sklearn\\utils\\_set_output.py\", line 316, in wrapped\n",
      "    data_to_wrap = f(self, X, *args, **kwargs)\n",
      "  File \"C:\\Users\\amitk\\AppData\\Roaming\\Python\\Python313\\site-packages\\sklearn\\base.py\", line 1365, in wrapper\n",
      "    return fit_method(estimator, *args, **kwargs)\n",
      "  File \"C:\\Users\\amitk\\AppData\\Roaming\\Python\\Python313\\site-packages\\sklearn\\decomposition\\_pca.py\", line 466, in fit_transform\n",
      "    U, S, _, X, x_is_centered, xp = self._fit(X)\n",
      "                                    ~~~~~~~~~^^^\n",
      "  File \"C:\\Users\\amitk\\AppData\\Roaming\\Python\\Python313\\site-packages\\sklearn\\decomposition\\_pca.py\", line 540, in _fit\n",
      "    return self._fit_full(X, n_components, xp, is_array_api_compliant)\n",
      "           ~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\amitk\\AppData\\Roaming\\Python\\Python313\\site-packages\\sklearn\\decomposition\\_pca.py\", line 554, in _fit_full\n",
      "    raise ValueError(\n",
      "    ...<3 lines>...\n",
      "    )\n",
      "ValueError: n_components=100 must be between 0 and min(n_samples, n_features)=80 with svd_solver='full'\n",
      "\n",
      "  warnings.warn(some_fits_failed_message, FitFailedWarning)\n",
      "C:\\Users\\amitk\\AppData\\Roaming\\Python\\Python313\\site-packages\\sklearn\\model_selection\\_search.py:1135: UserWarning: One or more of the test scores are non-finite: [0.44 0.43 0.34  nan 0.38 0.41 0.39  nan 0.4  0.4  0.37  nan 0.4  0.4\n",
      " 0.37  nan 0.39 0.39 0.37  nan 0.48 0.37 0.36  nan]\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best params: {'knn__metric': 'manhattan', 'knn__n_neighbors': 7, 'pca__n_components': 10}\n",
      "Best CV accuracy: 0.4800\n",
      "\n",
      "Selected number of components: 10\n",
      "Explained variance ratio sum for selected components: 0.1410\n"
     ]
    }
   ],
   "source": [
    "# Demo: synthetic high-dimensional gene expression-like data\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import GridSearchCV, StratifiedKFold\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "# Create synthetic dataset: 100 samples, 2000 features, 10 informative\n",
    "X_hd, y_hd = make_classification(n_samples=100, n_features=2000, n_informative=10, n_redundant=50, n_classes=3, random_state=42)\n",
    "print('Synthetic high-dim data shape:', X_hd.shape)\n",
    "\n",
    "# Pipeline: scaler -> PCA -> KNN with cross-validation to choose n_components and k\n",
    "pipe = Pipeline([\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('pca', PCA()),\n",
    "    ('knn', KNeighborsClassifier())\n",
    "])\n",
    "\n",
    "param_grid = {\n",
    "    'pca__n_components': [10, 20, 50, 100],\n",
    "    'knn__n_neighbors': [3,5,7],\n",
    "    'knn__metric': ['euclidean', 'manhattan']\n",
    "}\n",
    "\n",
    "cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "search = GridSearchCV(pipe, param_grid, cv=cv, scoring='accuracy', n_jobs=-1)\n",
    "search.fit(X_hd, y_hd)\n",
    "\n",
    "print('Best params:', search.best_params_)\n",
    "print('Best CV accuracy: {:.4f}'.format(search.best_score_))\n",
    "\n",
    "# Show how many components were chosen and why\n",
    "best_pca_n = search.best_params_['pca__n_components']\n",
    "print('\\nSelected number of components:', best_pca_n)\n",
    "\n",
    "# Fit PCA separately to show explained variance for that number\n",
    "scaler = StandardScaler().fit(X_hd)\n",
    "X_hd_s = scaler.transform(X_hd)\n",
    "pca_best = PCA(n_components=best_pca_n).fit(X_hd_s)\n",
    "print('Explained variance ratio sum for selected components: {:.4f}'.format(pca_best.explained_variance_ratio_.sum()))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
