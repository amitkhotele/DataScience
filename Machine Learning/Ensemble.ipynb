{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "be74383d",
   "metadata": {},
   "source": [
    "#Question 1: What is Ensemble Learning in machine learning? Explain the key idea\n",
    "behind it.\n",
    "Ensemble learning combines multiple base models (often called learners) to produce a single aggregated prediction that is usually more accurate and robust than any individual model.\n",
    "Key idea: different models make different errors; by combining them intelligently (averaging, voting, weighted sums, sequential corrections), the ensemble reduces variance, bias or both — improving generalization. Common paradigms: bagging (reduce variance), boosting (reduce bias), stacking (learn how to combine models).\n",
    "\n",
    "#Question 2: What is the difference between Bagging and Boosting?\n",
    "Bagging (Bootstrap AGGregatING)\n",
    "Idea: train many independent base learners on different bootstrap samples (sampling with replacement) of the training set, then aggregate by averaging (regression) or majority vote (classification).\n",
    "Effect: reduces variance (stabilizes high-variance models such as decision trees).\n",
    "Base learners trained in parallel (independent).\n",
    "Examples: Random Forest (bagging + feature subsampling).\n",
    "\n",
    "Boosting\n",
    "Idea: train base learners sequentially; each new learner focuses on examples the previous ones handled poorly (via reweighting or residual fitting). The ensemble aggregates learners (often weighted).\n",
    "Effect: reduces bias and can also reduce variance; produces a strong learner from many weak ones.\n",
    "More prone to overfitting if not regularized, but often very powerful.\n",
    "Examples: AdaBoost, Gradient Boosting Machines (GBM), XGBoost, LightGBM, CatBoost.\n",
    "\n",
    "#Question 3: What is bootstrap sampling and what role does it play in Bagging methods\n",
    "like Random Forest?\n",
    "Bootstrap sampling is sampling with replacement from the original dataset to create multiple different training sets (each the same size as the original). Each bootstrap sample will contain about ~63.2% unique original examples on average (the rest are duplicates), leaving ~36.8% not selected.\n",
    "\n",
    "Role in Bagging / Random Forest:\n",
    "Creates diverse training sets so base learners (e.g., trees) differ from one another.\n",
    "Diversity reduces correlation among base learners; averaging reduces variance.\n",
    "Random Forest adds an extra randomness layer by subsampling features at each split (feature bagging), further decorrelating trees.\n",
    "\n",
    "#Question 4: What are Out-of-Bag (OOB) samples and how is OOB score used to\n",
    "evaluate ensemble models?\n",
    "Out-of-Bag (OOB) samples: For a given bootstrap sample, the examples not included (roughly 36.8% of original data) are OOB for that particular base learner.\n",
    "OOB score (evaluation): For ensemble methods that use bootstrapping (like Random Forest), you can compute predictions for each training sample using only those trees for which the sample was OOB. Aggregating those predictions (vote/average) gives an OOB estimate of performance (accuracy, MSE, etc.) without needing an external validation set or cross-validation.\n",
    "Practical: OOB score is a fast built-in estimate of generalization performance and is often close to cross-validated performance for Random Forest.\n",
    "\n",
    "#Question 5: Compare feature importance analysis in a single Decision Tree vs. a\n",
    "Random Forest.\n",
    "Single Decision Tree\n",
    "Feature importance often computed as the total decrease in impurity (Gini/Entropy/MSE) brought by splits on that feature, summed over the tree.\n",
    "Can be unstable: small changes in data or hyperparameters may change which feature is chosen for splits, so importances can vary a lot.\n",
    "\n",
    "Random Forest\n",
    "Feature importance is averaged across all trees (mean decrease in impurity) — much more stable.\n",
    "Random Forest can also compute permutation importance (measuring drop in performance when a feature is shuffled) which is model-agnostic and often more reliable for ranking.\n",
    "Because Random Forest averages many trees (each built on different data+feature subsets), the importances are more robust and less sensitive to noise and correlated features. However correlated features can still split importance among themselves reducing per-feature scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4e840a44",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 5 features:\n",
      "worst area: 0.149674\n",
      "worst concave points: 0.127189\n",
      "mean concave points: 0.104650\n",
      "worst radius: 0.086963\n",
      "worst perimeter: 0.080299\n"
     ]
    }
   ],
   "source": [
    "'''#Question 6: Write a Python program to:\n",
    "● Load the Breast Cancer dataset using\n",
    "sklearn.datasets.load_breast_cancer()\n",
    "● Train a Random Forest Classifier\n",
    "● Print the top 5 most important features based on feature importance scores.\n",
    "(Include your Python code and output in the code box below.)'''\n",
    "\n",
    "# Q6: Random Forest on Breast Cancer dataset, print top 5 features\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "\n",
    "data = load_breast_cancer()\n",
    "X, y = data.data, data.target\n",
    "feature_names = data.feature_names\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=42, stratify=y)\n",
    "\n",
    "rf = RandomForestClassifier(n_estimators=100, random_state=42, n_jobs=-1)\n",
    "rf.fit(X_train, y_train)\n",
    "\n",
    "importances = rf.feature_importances_\n",
    "indices = np.argsort(importances)[::-1]\n",
    "\n",
    "print(\"Top 5 features:\")\n",
    "for i in indices[:5]:\n",
    "    print(f\"{feature_names[i]}: {importances[i]:.6f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9d16daf7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Q7: BaggingClassifier (Decision Trees) on Iris dataset ===\n",
      "Single Decision Tree accuracy: 0.8947\n",
      "Bagging (25 trees) accuracy: 0.9474\n"
     ]
    }
   ],
   "source": [
    "# Q7: Bagging Classifier vs single Decision Tree on Iris\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Load Iris dataset\n",
    "iris = load_iris()\n",
    "X, y = iris.data, iris.target\n",
    "\n",
    "# Split into training and testing\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=42, stratify=y)\n",
    "\n",
    "# Train a single Decision Tree\n",
    "dt = DecisionTreeClassifier(random_state=42)\n",
    "dt.fit(X_train, y_train)\n",
    "dt_acc = accuracy_score(y_test, dt.predict(X_test))\n",
    "\n",
    "# ✅ Bagging with Decision Trees (use 'estimator' instead of 'base_estimator')\n",
    "bag = BaggingClassifier(estimator=DecisionTreeClassifier(),\n",
    "                        n_estimators=25,\n",
    "                        random_state=42,\n",
    "                        n_jobs=-1)\n",
    "bag.fit(X_train, y_train)\n",
    "bag_acc = accuracy_score(y_test, bag.predict(X_test))\n",
    "\n",
    "print(\"=== Q7: BaggingClassifier (Decision Trees) on Iris dataset ===\")\n",
    "print(f\"Single Decision Tree accuracy: {dt_acc:.4f}\")\n",
    "print(f\"Bagging (25 trees) accuracy: {bag_acc:.4f}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c9e18fa4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters found: {'max_depth': None, 'n_estimators': 100}\n",
      "Best CV accuracy: 0.9601\n",
      "Test set accuracy with best params: 0.9510\n"
     ]
    }
   ],
   "source": [
    "# Q8: Random Forest + GridSearchCV (tune max_depth and n_estimators) on Breast Cancer\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import GridSearchCV, train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "data = load_breast_cancer()\n",
    "X, y = data.data, data.target\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=7, stratify=y)\n",
    "\n",
    "rf = RandomForestClassifier(random_state=42)\n",
    "param_grid = {\n",
    "    \"n_estimators\": [50, 100],    # small grid for speed; expand if needed\n",
    "    \"max_depth\": [None, 5, 10]\n",
    "}\n",
    "\n",
    "grid = GridSearchCV(rf, param_grid, cv=3, n_jobs=-1)\n",
    "grid.fit(X_train, y_train)\n",
    "\n",
    "best_params = grid.best_params_\n",
    "best_cv_score = grid.best_score_\n",
    "test_acc = accuracy_score(y_test, grid.best_estimator_.predict(X_test))\n",
    "\n",
    "print(\"Best parameters found:\", best_params)\n",
    "print(f\"Best CV accuracy: {best_cv_score:.4f}\")\n",
    "print(f\"Test set accuracy with best params: {test_acc:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ab0735ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Q9: Bagging Regressor vs Random Forest Regressor on California Housing ===\n",
      "Bagging Regressor MSE: 0.2648\n",
      "Random Forest Regressor MSE: 0.2542\n"
     ]
    }
   ],
   "source": [
    "# Q9: Bagging Regressor and Random Forest Regressor on California housing\n",
    "from sklearn.datasets import fetch_california_housing\n",
    "from sklearn.ensemble import BaggingRegressor, RandomForestRegressor\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# Load California Housing dataset\n",
    "cal = fetch_california_housing()\n",
    "X, y = cal.data, cal.target\n",
    "\n",
    "# Split data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=42)\n",
    "\n",
    "# ✅ Use 'estimator' instead of 'base_estimator'\n",
    "bag_reg = BaggingRegressor(estimator=DecisionTreeRegressor(),\n",
    "                           n_estimators=20,\n",
    "                           random_state=42,\n",
    "                           n_jobs=-1)\n",
    "\n",
    "rf_reg = RandomForestRegressor(n_estimators=100, random_state=42, n_jobs=-1)\n",
    "\n",
    "# Train both models\n",
    "bag_reg.fit(X_train, y_train)\n",
    "rf_reg.fit(X_train, y_train)\n",
    "\n",
    "# Predictions\n",
    "bag_pred = bag_reg.predict(X_test)\n",
    "rf_pred = rf_reg.predict(X_test)\n",
    "\n",
    "# Evaluate with Mean Squared Error (MSE)\n",
    "mse_bag = mean_squared_error(y_test, bag_pred)\n",
    "mse_rf = mean_squared_error(y_test, rf_pred)\n",
    "\n",
    "print(\"=== Q9: Bagging Regressor vs Random Forest Regressor on California Housing ===\")\n",
    "print(f\"Bagging Regressor MSE: {mse_bag:.4f}\")\n",
    "print(f\"Random Forest Regressor MSE: {mse_rf:.4f}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38e5d936",
   "metadata": {},
   "source": [
    "#Question 10: You are working as a data scientist at a financial institution to predict loan\n",
    "default. You have access to customer demographic and transaction history data.\n",
    "You decide to use ensemble techniques to increase model performance.\n",
    "Explain your step-by-step approach to:\n",
    "● Choose between Bagging or Boosting\n",
    "● Handle overfitting\n",
    "● Select base models\n",
    "● Evaluate performance using cross-validation\n",
    "● Justify how ensemble learning improves decision-making in this real-world\n",
    "context.\n",
    "\n",
    "Step 1: Choosing Between Bagging and Boosting\n",
    "\n",
    "Bagging (e.g., Random Forest): Reduces variance; good for unstable models like Decision Trees.\n",
    "\n",
    "Boosting (e.g., XGBoost, LightGBM): Reduces bias; trains models sequentially to fix previous errors.\n",
    "✅ For loan default prediction, Boosting is preferred — it handles class imbalance and captures complex patterns.\n",
    "\n",
    "Step 2: Handling Overfitting\n",
    "\n",
    "Use cross-validation for model validation.\n",
    "\n",
    "Apply hyperparameter tuning (e.g., max_depth, learning_rate).\n",
    "\n",
    "Use regularization and early stopping.\n",
    "\n",
    "Balance data using class weights or SMOTE.\n",
    "\n",
    "Step 3: Selecting Base Models\n",
    "\n",
    "Try Decision Trees, Logistic Regression, or XGBoost.\n",
    "\n",
    "Choose the model with the best cross-validated ROC-AUC or F1-score.\n",
    "\n",
    "Step 4: Model Evaluation\n",
    "\n",
    "Use k-fold cross-validation and metrics like:\n",
    "\n",
    "Accuracy\n",
    "\n",
    "Precision/Recall\n",
    "\n",
    "ROC-AUC\n",
    "\n",
    "Step 5: Why Ensemble Learning Helps\n",
    "\n",
    "Combines multiple weak learners for better accuracy.\n",
    "\n",
    "Reduces bias & variance → more stable predictions.\n",
    "\n",
    "Improves risk prediction → helps financial institutions make safer loan approvals."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fd49818c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average ROC-AUC: 0.9909759807769962\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "# Load classification dataset\n",
    "data = load_breast_cancer()\n",
    "X, y = data.data, data.target\n",
    "\n",
    "# Train model and evaluate using ROC-AUC\n",
    "model = GradientBoostingClassifier()\n",
    "scores = cross_val_score(model, X, y, cv=5, scoring='roc_auc')\n",
    "print(\"Average ROC-AUC:\", scores.mean())\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
