{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fb199eef",
   "metadata": {},
   "source": [
    "# Decision Trees\n",
    "\n",
    "---\n",
    "\n",
    "## Q1: What is a Decision Tree, and how does it work in the context of classification?\n",
    "\n",
    "A Decision Tree is a supervised learning algorithm used for classification and regression tasks. It splits the data into subsets based on feature values, forming a tree-like structure where each node represents a decision based on a feature, and each leaf node represents a class label (for classification). The tree recursively partitions the data to maximize the separation between classes.\n",
    "\n",
    "---\n",
    "\n",
    "## Q2: Explain the concepts of Gini Impurity and Entropy as impurity measures. How do they impact the splits in a Decision Tree?\n",
    "\n",
    "- **Gini Impurity** measures the probability of incorrectly classifying a randomly chosen element if it was randomly labeled according to the distribution of labels in the subset.\n",
    "- **Entropy** quantifies the amount of disorder or uncertainty in the subset.\n",
    "\n",
    "Both are used to evaluate the quality of splits: lower impurity means better separation. The algorithm chooses splits that minimize impurity (Gini or Entropy), resulting in purer child nodes.\n",
    "\n",
    "---\n",
    "\n",
    "## Q3: What is the difference between Pre-Pruning and Post-Pruning in Decision Trees? Give one practical advantage of using each.\n",
    "\n",
    "- **Pre-Pruning** stops the tree growth early (e.g., by setting max_depth, min_samples_split) to prevent overfitting.\n",
    "    - *Advantage:* Reduces computation and risk of overfitting.\n",
    "- **Post-Pruning** grows the tree fully and then removes branches that do not improve performance.\n",
    "    - *Advantage:* Can yield a simpler, more generalizable model after evaluating the full tree.\n",
    "\n",
    "---\n",
    "\n",
    "## Q4: What is Information Gain in Decision Trees, and why is it important for choosing the best split?\n",
    "\n",
    "Information Gain measures the reduction in impurity (Entropy or Gini) after a dataset is split on a feature. It helps select the feature and threshold that best separates the classes, leading to more informative splits and improved model accuracy.\n",
    "\n",
    "---\n",
    "\n",
    "## Q5: What are some common real-world applications of Decision Trees, and what are their main advantages and limitations?\n",
    "\n",
    "**Applications:** Medical diagnosis, credit scoring, customer segmentation, fraud detection.\n",
    "\n",
    "**Advantages:** Easy to interpret, handle both numerical and categorical data, require little data preprocessing.\n",
    "\n",
    "**Limitations:** Prone to overfitting, unstable to small data changes, may not capture complex relationships as well as ensemble methods.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cd9735cf",
   "metadata": {
    "vscode": {
     "languageId": "markdown"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 1.0\n",
      "Feature Importances: [0.01333333 0.         0.56405596 0.42261071]\n"
     ]
    }
   ],
   "source": [
    "## Q6: Python Program - Iris Dataset, Decision Tree Classifier (Gini)\n",
    "\n",
    "\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "iris = load_iris()\n",
    "X, y = iris.data, iris.target\n",
    "\n",
    "clf = DecisionTreeClassifier(criterion='gini', random_state=42)\n",
    "clf.fit(X, y)\n",
    "y_pred = clf.predict(X)\n",
    "\n",
    "print(\"Accuracy:\", accuracy_score(y, y_pred))\n",
    "print(\"Feature Importances:\", clf.feature_importances_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fe862418",
   "metadata": {
    "vscode": {
     "languageId": "markdown"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fully-grown tree accuracy: 1.0\n",
      "max_depth=3 tree accuracy: 1.0\n"
     ]
    }
   ],
   "source": [
    "## Q7: Python Program - Compare max_depth=3 vs Fully-Grown Tree\n",
    "\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42)\n",
    "\n",
    "# Fully-grown tree\n",
    "clf_full = DecisionTreeClassifier(random_state=42)\n",
    "clf_full.fit(X_train, y_train)\n",
    "acc_full = clf_full.score(X_test, y_test)\n",
    "\n",
    "# Pruned tree\n",
    "clf_pruned = DecisionTreeClassifier(max_depth=3, random_state=42)\n",
    "clf_pruned.fit(X_train, y_train)\n",
    "acc_pruned = clf_pruned.score(X_test, y_test)\n",
    "\n",
    "print(\"Fully-grown tree accuracy:\", acc_full)\n",
    "print(\"max_depth=3 tree accuracy:\", acc_pruned)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "21a51f97",
   "metadata": {
    "vscode": {
     "languageId": "markdown"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Squared Error: 9.555001274479309e-32\n",
      "Feature Importances: [0.52500998 0.05100488 0.05341707 0.02651485 0.03282405 0.1320936\n",
      " 0.09387213 0.08526344]\n"
     ]
    }
   ],
   "source": [
    "## Q8: Python Program - California Housing, Decision Tree Regressor\n",
    "\n",
    "from sklearn.datasets import fetch_california_housing\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "housing = fetch_california_housing()\n",
    "X_housing, y_housing = housing.data, housing.target\n",
    "\n",
    "reg = DecisionTreeRegressor(random_state=42)\n",
    "reg.fit(X_housing, y_housing)\n",
    "y_pred = reg.predict(X_housing)\n",
    "\n",
    "print(\"Mean Squared Error:\", mean_squared_error(y_housing, y_pred))\n",
    "print(\"Feature Importances:\", reg.feature_importances_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "16260aea",
   "metadata": {
    "vscode": {
     "languageId": "markdown"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Parameters: {'max_depth': 3, 'min_samples_split': 2}\n",
      "Best Accuracy: 0.9733333333333334\n"
     ]
    }
   ],
   "source": [
    "## Q9: Python Program - GridSearchCV for Decision Tree Hyperparameters\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "param_grid = {\n",
    "        'max_depth': [2, 3, 4, 5, None],\n",
    "        'min_samples_split': [2, 5, 10]\n",
    "}\n",
    "\n",
    "grid = GridSearchCV(DecisionTreeClassifier(random_state=42), param_grid, cv=5)\n",
    "grid.fit(X, y)\n",
    "\n",
    "print(\"Best Parameters:\", grid.best_params_)\n",
    "print(\"Best Accuracy:\", grid.best_score_)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "177f8d44",
   "metadata": {},
   "source": [
    "## Q10: Step-by-Step Process for Healthcare Classification Task\n",
    "\n",
    "1. **Handle Missing Values:** Impute missing values using mean/median for numerical features, mode or a separate category for categorical features.\n",
    "2. **Encode Categorical Features:** Use one-hot encoding or ordinal encoding as appropriate.\n",
    "3. **Train Decision Tree Model:** Split data into train/test sets, fit a Decision Tree classifier.\n",
    "4. **Tune Hyperparameters:** Use GridSearchCV or RandomizedSearchCV to optimize parameters like max_depth, min_samples_split.\n",
    "5. **Evaluate Performance:** Assess accuracy, precision, recall, F1-score, and ROC-AUC on test data.\n",
    "\n",
    "**Business Value:** The model can help identify high-risk patients, enabling early intervention, personalized treatment, and resource optimization, ultimately improving patient outcomes and reducing costs."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
